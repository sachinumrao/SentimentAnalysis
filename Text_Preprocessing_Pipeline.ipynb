{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Data/movie_reviews.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(filename, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first think another Disney movie, might good, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big fan Stephen King's work, film made even gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       SentimentText  Sentiment\n",
       "0  first think another Disney movie, might good, ...          1\n",
       "1  Put aside Dr. House repeat missed, Desperate H...          0\n",
       "2  big fan Stephen King's work, film made even gr...          1\n",
       "3  watched horrid thing TV. Needless say one movi...          0\n",
       "4  truly enjoyed film. acting terrific plot. Jeff...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_words = ['cannot', 'never', 'no', 'not']\n",
    "for j in retain_words:\n",
    "    stops.discard(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = ['br', 'a', 'b' 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q',\n",
    "               'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "for j in remove_words:\n",
    "    stops.add(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_tokenizer(text, stops):\n",
    "    # Fix contractions\n",
    "    text2 = contractions.fix(text)\n",
    "    # Tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text2)\n",
    "    # Remove numbers\n",
    "    words2 = [x for x in words if x.isdigit()==False]\n",
    "    # Convert into lowercase\n",
    "    words3 = [x.lower() for x in words2]\n",
    "    # Remove stopwords\n",
    "    words4 = []\n",
    "    for w in words3:\n",
    "        if w in stops:\n",
    "            continue\n",
    "        else:\n",
    "            words4.append(w)\n",
    "    return words4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows processed :  500\n",
      "Rows processed :  1000\n",
      "Rows processed :  1500\n",
      "Rows processed :  2000\n",
      "Rows processed :  2500\n",
      "Rows processed :  3000\n",
      "Rows processed :  3500\n",
      "Rows processed :  4000\n",
      "Rows processed :  4500\n",
      "Rows processed :  5000\n",
      "Rows processed :  5500\n",
      "Rows processed :  6000\n",
      "Rows processed :  6500\n",
      "Rows processed :  7000\n",
      "Rows processed :  7500\n",
      "Rows processed :  8000\n",
      "Rows processed :  8500\n",
      "Rows processed :  9000\n",
      "Rows processed :  9500\n",
      "Rows processed :  10000\n",
      "Rows processed :  10500\n",
      "Rows processed :  11000\n",
      "Rows processed :  11500\n",
      "Rows processed :  12000\n",
      "Rows processed :  12500\n",
      "Rows processed :  13000\n",
      "Rows processed :  13500\n",
      "Rows processed :  14000\n",
      "Rows processed :  14500\n",
      "Rows processed :  15000\n",
      "Rows processed :  15500\n",
      "Rows processed :  16000\n",
      "Rows processed :  16500\n",
      "Rows processed :  17000\n",
      "Rows processed :  17500\n",
      "Rows processed :  18000\n",
      "Rows processed :  18500\n",
      "Rows processed :  19000\n",
      "Rows processed :  19500\n",
      "Rows processed :  20000\n",
      "Rows processed :  20500\n",
      "Rows processed :  21000\n",
      "Rows processed :  21500\n",
      "Rows processed :  22000\n",
      "Rows processed :  22500\n",
      "Rows processed :  23000\n",
      "Rows processed :  23500\n",
      "Rows processed :  24000\n",
      "Rows processed :  24500\n",
      "Rows processed :  25000\n"
     ]
    }
   ],
   "source": [
    "docs_list = []\n",
    "for j in range(data.shape[0]):\n",
    "    word_list = regex_tokenizer(data['SentimentText'][j], stops)\n",
    "    docs_list.append(word_list)\n",
    "    if (j+1)%500 == 0:\n",
    "        print(\"Rows processed : \",j+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents:  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents: \", len(docs_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make word_index map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Value of Counter : 74860\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "length_list = np.zeros((data.shape[0],1))\n",
    "counter = 1\n",
    "id_empty_sents = []\n",
    "for index,word_list in enumerate(docs_list):\n",
    "    length_list[index] = len(word_list)\n",
    "    if len(word_list) == 0:\n",
    "        print(\"Index : \",index)\n",
    "    for aword in word_list:\n",
    "        if aword in word_to_id:\n",
    "            continue\n",
    "        else:\n",
    "            word_to_id[aword] = counter\n",
    "            id_to_word[counter] = aword\n",
    "            counter = counter + 1\n",
    "            \n",
    "print(\"Final Value of Counter :\", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Sentence Length : 78.0\n",
      "Mean Sentence Length : 105.49736\n",
      "Max Sentence Length : 1282.0\n",
      "Min Sentence Length : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Median Sentence Length :\", np.median(length_list))\n",
    "print(\"Mean Sentence Length :\", np.mean(length_list))\n",
    "print(\"Max Sentence Length :\", np.max(length_list))\n",
    "print(\"Min Sentence Length :\", np.min(length_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8177\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "ll = word_to_id['hello']\n",
    "print(ll)\n",
    "lm = id_to_word[8177]\n",
    "print(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-82341acf7437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'k'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
